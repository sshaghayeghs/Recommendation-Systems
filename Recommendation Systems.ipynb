{
 "cells": [
  {
   "cell_type": "markdown",
   "metadata": {},
   "source": [
    "<img src=\"image/0.PNG\" width=\"1000\" height=\"200\">\n",
    "\n",
    "# Building Recommendation Systems in Python\n",
    "\n",
    "**Date:** June 03, 2022 \n",
    "\n",
    "**Presenter:** Shaghayegh Sadeghi\n",
    "\n",
    "## workshop description\n",
    "In this course, students will learn everything they need to know to create their own recommendation engine. Through hands-on exercises, students will get to grips with the two most common systems, collaborative filtering, and content-based filtering. Next, students will learn how to measure similarities like the Jaccard distance and cosine similarity, and how to evaluate the quality of recommendations on test data using the root mean square error (RMSE).\n",
    "\n",
    "**Learning Objectives/Outcomes:** By the end of this course, students will have built their very own movie recommendation engine and be able to apply their Python skills to create these systems for any industry.\n",
    "<img src=\"image/c1.PNG\" width=\"1000\" height=\"200\">\n",
    "\n",
    "<img src=\"https://files.realpython.com/media/Build-a-Recommendation-Engine-With-Collaborative-Filtering_Watermarked.451abc4ecb9f.jpg\" width=\"500\" height=\"500\">\n",
    "\n",
    "\n",
    " **Impact of Recommender systems:**\n",
    " \n",
    " 40% of apps installed in Google Play\n",
    " \n",
    " 60% of watch time in YouTube\n",
    " \n",
    " 35% of purchases in Amzaon\n",
    " \n",
    " 75% of movies watched on NETFLIX\n",
    " \n",
    " ### What are recommendation engines?\n",
    " \n",
    " <img src=\"image/1.PNG\" width=\"500\" height=\"100\">\n",
    "\n",
    "### What kind of data do I need?\n",
    "\n",
    "Recommendation engines use the feedback of users to find new relevant items for them or for others with the assumption that users who have similar preferences in the past are likely to have similar preferences in the future like the example here. Recommendation engines benefit from having a many to many match between the users giving the feedback, and the items receiving the feedback. In other words, a better recommendation can be made for an item that has been given a lot of feedback, and more personalized recommendations can be given for a user that has given a lot of feedback.\n",
    "\n",
    "  <img src=\"image/4.PNG\" width=\"500\" height=\"100\">\n",
    "\n",
    "  <img src=\"image/2.PNG\" width=\"500\" height=\"100\">\n",
    "\n",
    " \n",
    "### What are recommendation engines useful for?\n",
    "**Recommendation Engines**\n",
    "\n",
    "What movie should a viewer watch?\n",
    "\n",
    "Will a diner enjoy a restaurant?\n",
    "\n",
    "**Other Statistical Models**\n",
    "\n",
    "Will a movie sell a lot of tickets?\n",
    "\n",
    "How much is a house worth?\n",
    "\n",
    "\n",
    "  <img src=\"image/3.PNG\" width=\"1000\" height=\"200\">\n",
    "\n",
    "\n",
    "<img src=\"https://i1.wp.com/www.firstplaceforhealth.com/wp-content/uploads/2020/03/train-your-brain-gif-2.gif?fit=1200%2C904&ssl=1\" width=\"500\" height=\"100\">\n",
    "\n",
    "\n"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": null,
   "metadata": {},
   "outputs": [],
   "source": [
    "import pandas as pd\n",
    "import matplotlib.pyplot as plt"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": null,
   "metadata": {},
   "outputs": [],
   "source": [
    "#Implicit vs. explicit data\n",
    "listening_history_df=pd.read_csv('listening_history.csv')\n",
    "# Inspect the listening_history_df DataFrame\n",
    "print(listening_history_df.____())\n",
    "\n",
    "# Calculate the number of unique values\n",
    "print(listening_history_df[['Rating', 'Skipped Track']].____())\n",
    "\n",
    "# Display a histogram of the values in the Rating column\n",
    "listening_history_df['Rating'].____()\n",
    "plt.show()"
   ]
  },
  {
   "cell_type": "markdown",
   "metadata": {},
   "source": [
    "**Question**\n",
    "\n",
    "Which of its columns would be considered explicit data."
   ]
  },
  {
   "cell_type": "markdown",
   "metadata": {},
   "source": [
    "## Non-personalized recommendations\n",
    "They are made to all users, without taking their preferences into account.\n",
    "<img src=\"image/5.PNG\" width=\"800\" height=\"200\">\n",
    "\n",
    "#### Finding the most popular items\n",
    "`book_df` DataFrame:\n",
    "<img src=\"image/6.PNG\" width=\"400\" height=\"100\">\n",
    "```python\n",
    "book_df['book'].value_counts()\n",
    "```\n",
    "<img src=\"image/7.PNG\" width=\"500\" height=\"100\">\n",
    "\n",
    "```python\n",
    "print(book_df.value_counts().index)\n",
    "```\n",
    "<img src=\"image/8.PNG\" width=\"500\" height=\"100\">\n",
    "\n",
    "#### Finding the most liked items\n",
    "`user_ratings` DataFrame:\n",
    "<img src=\"image/9.PNG\" width=\"500\" height=\"100\">\n",
    "\n",
    "```python\n",
    "avg_rating_df = user_ratings[[\"book\", \"rating\"]].groupby(['book']).mean()\n",
    "avg_rating_df.head()\n",
    "```\n",
    "<img src=\"image/10.PNG\" width=\"500\" height=\"100\">\n",
    "\n",
    "```python\n",
    "sorted_avg_rating_df = avg_rating_df.sort_values(by=\"rating\", ascending=False)\n",
    "sorted_avg_rating_df.head()\n",
    "```\n",
    "<img src=\"image/11.PNG\" width=\"500\" height=\"100\">\n",
    "\n",
    "#### Finding the most liked popular items\n",
    "```python\n",
    "book_frequency = user_ratings[\"book\"].value_counts()\n",
    "print(book_frequency)\n",
    "```\n",
    "<img src=\"image/12.PNG\" width=\"500\" height=\"100\">\n",
    "\n",
    "```python\n",
    "frequently_reviewed_books = book_frequency[book_frequency > 100].index\n",
    "print(frequently_reviewed_books)\n",
    "```\n",
    "<img src=\"image/13.PNG\" width=\"500\" height=\"100\">\n",
    "\n",
    "```python\n",
    "frequent_books_df = user_ratings_df[user_ratings_df[\"book\"].isin(frequently_reviewed_books)]\n",
    "frequent_books_avgs = frequently_reviewed_books[[\"title\", \"rating\"]].groupby('title').mean()\n",
    "print(frequent_books_avgs.sort_values(by=\"rating\", ascending=False).head())\n",
    "```\n",
    "<img src=\"image/14.PNG\" width=\"500\" height=\"100\">\n",
    "\n",
    "<img src=\"https://i1.wp.com/www.firstplaceforhealth.com/wp-content/uploads/2020/03/train-your-brain-gif-2.gif?fit=1200%2C904&ssl=1\" width=\"300\" height=\"100\">\n",
    "\n",
    "**Exercise:** what is the most frequently watched movie overall in `user_rating` dataset?"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": null,
   "metadata": {},
   "outputs": [],
   "source": [
    "user_ratings_df=pd.read_csv('user_ratings.csv')\n",
    "user_ratings_df"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": null,
   "metadata": {},
   "outputs": [],
   "source": [
    "# Get the counts of occurrences of each movie title\n",
    "movie_popularity = user_ratings_df[\"title\"].____()\n",
    "movie_popularity"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": null,
   "metadata": {},
   "outputs": [],
   "source": [
    "print(movie_popularity.head().index)"
   ]
  },
  {
   "cell_type": "markdown",
   "metadata": {},
   "source": [
    "**Exercise:** you should find the average rating of each movie in the dataset, and then find the movie with the highest average rating."
   ]
  },
  {
   "cell_type": "code",
   "execution_count": null,
   "metadata": {},
   "outputs": [],
   "source": [
    "# Find the mean of the ratings given to each title\n",
    "average_rating_df = user_ratings_df[[\"title\", \"rating\"]].____('title').____()\n",
    "\n",
    "# Order the entries by highest average rating to lowest\n",
    "sorted_average_ratings = average_rating_df.____(____=____, ____=____)\n",
    "\n",
    "# Inspect the top movies\n",
    "print(____.____())"
   ]
  },
  {
   "cell_type": "markdown",
   "metadata": {},
   "source": [
    "**Exercise:** you will combine the two previous methods to find the average rating only for movies that have been reviewed more than 50 times."
   ]
  },
  {
   "cell_type": "code",
   "execution_count": null,
   "metadata": {},
   "outputs": [],
   "source": [
    "# Create a list of only the frequently watched movies\n",
    "movie_popularity = ____[\"title\"].____()\n",
    "popular_movies = ____[____ > 50].____\n",
    "\n",
    "print(popular_movies)"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": null,
   "metadata": {},
   "outputs": [],
   "source": [
    "# Use this popular_movies list to filter the original DataFrame\n",
    "popular_movies_rankings = user_ratings_df[user_ratings_df[\"title\"].____(____)]\n",
    "\n",
    "# Inspect the movies watched over 50 times\n",
    "print(popular_movies_rankings)"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": null,
   "metadata": {},
   "outputs": [],
   "source": [
    "# Find the average rating given to these frequently watched films\n",
    "popular_movies_average_rankings = popular_movies_rankings[[\"title\", \"rating\"]].____('title').____()\n",
    "print(popular_movies_average_rankings.____(____=\"rating\", ascending=____).head())"
   ]
  },
  {
   "cell_type": "markdown",
   "metadata": {},
   "source": [
    "## Non-personalized suggestions\n",
    "\n",
    "### Identifying pairs\n",
    "<img src=\"image/15.PNG\" width=\"500\" height=\"100\">\n",
    "\n",
    "### Permutations versus combinations\n",
    "<img src=\"image/16.PNG\" width=\"500\" height=\"100\">\n",
    "\n",
    "Books seen with `The Great Gatsby` -> `The Catcher in the Rye`\n",
    "\n",
    "Books seen with `The Catcher in the Rye` -> `The Great Gatsby`\n",
    "### Creating the pairing function\n",
    "``` python\n",
    "from itertools import permutations\n",
    "def create_pairs(x):\n",
    "    pairs = pd.DataFrame(list(permutations(x.values, 2)),\n",
    "    columns=['book_a','book_b'])\n",
    "return pairs\n",
    "```\n",
    "- `permutations(list, length_of_permutations))` Generates iterable object containing all permutations\n",
    "- `list()` Converts this object to a usable list\n",
    "- `pd.DataFrame()` Converts the list to a DataFrame containing the columns `book_a` and `book_b`\n",
    "\n",
    "```python\n",
    "#Applying the function to the data\n",
    "book_pairs = book_df.groupby('userId')['book_title'].apply(perm_function)\n",
    "print(book_pairs.head())\n",
    "```\n",
    "<img src=\"image/17.PNG\" width=\"500\" height=\"100\">\n",
    "\n",
    "```python\n",
    "#Cleaning up the results\n",
    "book_pairs = book_pairs.reset_index(drop=True)\n",
    "print(book_pairs.head())\n",
    "```\n",
    "<img src=\"image/18.PNG\" width=\"500\" height=\"100\">\n",
    "\n",
    "```python\n",
    "#Counting the pairings\n",
    "pair_counts = book_pairs.groupby(['book_a', 'book_b']).size()\n",
    "pair_counts_df = pair_counts.to_frame(name = 'size').reset_index()\n",
    "print(pair_counts_df.head())\n",
    "```\n",
    "<img src=\"image/20.PNG\" width=\"500\" height=\"100\">\n",
    "\n",
    "```python\n",
    "#Looking up recommendations\n",
    "pair_counts_sorted = pair_counts_df.sort_values('size', ascending=False)\n",
    "pair_counts_sorted[pair_counts_sorted['book_a'] == 'Lord of the Rings']\n",
    "```\n",
    "<img src=\"image/21.PNG\" width=\"500\" height=\"100\">\n"
   ]
  },
  {
   "cell_type": "markdown",
   "metadata": {},
   "source": [
    "<img src=\"https://i1.wp.com/www.firstplaceforhealth.com/wp-content/uploads/2020/03/train-your-brain-gif-2.gif?fit=1200%2C904&ssl=1\" width=\"300\" height=\"100\">\n",
    "\n",
    "**Exercise** Work through how to find all pairs of movies or all permutations of pairs of movies that have been watched by the same person."
   ]
  },
  {
   "cell_type": "code",
   "execution_count": null,
   "metadata": {},
   "outputs": [],
   "source": [
    "from itertools import permutations\n",
    "\n",
    "# Create the function to find all permutations\n",
    "def ____(x):\n",
    "  pairs = pd.____(list(____(x.values, 2)),\n",
    "                       columns=['movie_a', 'movie_b'])\n",
    "  return pairs\n",
    "\n",
    "# Apply the function to the title column and reset the index\n",
    "movie_combinations = user_ratings_df.____('userId')['title'].____(find_movie_pairs)\n",
    "\n",
    "print(movie_combinations)"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": null,
   "metadata": {},
   "outputs": [],
   "source": [
    "# Apply the function to the title column and reset the index\n",
    "movie_combinations = movie_combinations.____(____=____)\n",
    "\n",
    "print(movie_combinations)"
   ]
  },
  {
   "cell_type": "markdown",
   "metadata": {},
   "source": [
    "**Exercise:** Work with the movie_combinations DataFrame that you created in the last exercise, and generate a new DataFrame containing the counts of occurrences of each of the pairs within."
   ]
  },
  {
   "cell_type": "code",
   "execution_count": null,
   "metadata": {},
   "outputs": [],
   "source": [
    "# Calculate how often each item in movies_a occurs with the items in movies_b\n",
    "combination_counts = movie_combinations.____(['movie_a', 'movie_b']).____()\n",
    "\n",
    "# Inspect the results\n",
    "print(combination_counts.head())"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": null,
   "metadata": {},
   "outputs": [],
   "source": [
    "# Convert the results to a DataFrame and reset the index\n",
    "combination_counts_df = combination_counts.____(____=____).____()\n",
    "print(combination_counts_df.head())"
   ]
  },
  {
   "cell_type": "markdown",
   "metadata": {},
   "source": [
    "**Exercise:** Making your first movie recommendations"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": null,
   "metadata": {},
   "outputs": [],
   "source": [
    "# Sort the counts from highest to lowest\n",
    "combination_counts_df.____('size', ascending=____)\n",
    "\n",
    "# Find the movies most frequently watched by people who watched Thor\n",
    "thor_df = ____[____['movie_a'] ____ 'Toy Story (1995)']\n"
   ]
  },
  {
   "cell_type": "markdown",
   "metadata": {},
   "source": [
    "<img src=\"image/c2.PNG\" width=\"1000\" height=\"100\">\n"
   ]
  },
  {
   "cell_type": "markdown",
   "metadata": {},
   "source": [
    "## What are content-based recommendations?\n",
    "we will move to more targeted models by recommending items based on their similarities to items a user has liked in the past. The recommendations made by finding items with similar attributes are called content-based recommendations.\n",
    "\n",
    "<img src=\"image/22.PNG\" width=\"500\" height=\"100\">\n",
    "\n",
    "### Items' attributes or characteristics\n",
    "A big advantage of using an item's attributes over user feedback is that you can make recommendations for any items you have attribute data on. This includes even brand new items that users have not seen yet. Content-based models require us to use any available attributes to build profiles of items in a way that allows us to mathematically compare between them. This allows us for example to find the most similar items and recommend them.\n",
    "\n",
    "<img src=\"image/23.PNG\" width=\"500\" height=\"100\">\n",
    "\n",
    "### Vectorizing your attributes\n",
    "This is best done by encoding each item as a vector. It is extremely valuable to have your data in this format so the distance and similarities between items can be easily calculated, which is vital for generating recommendations. \n",
    "<img src=\"image/24.PNG\" width=\"500\" height=\"100\">\n",
    "<img src=\"image/25.PNG\" width=\"500\" height=\"100\">\n",
    "\n",
    "\n",
    "### Crosstabulation\n",
    "\n",
    "```python\n",
    "pd.crosstab(book_genre_df['Book'], book_genre_df['Genre'])\n",
    "```\n",
    "<img src=\"image/26.PNG\" width=\"500\" height=\"100\">\n"
   ]
  },
  {
   "cell_type": "markdown",
   "metadata": {},
   "source": [
    "<img src=\"https://i1.wp.com/www.firstplaceforhealth.com/wp-content/uploads/2020/03/train-your-brain-gif-2.gif?fit=1200%2C904&ssl=1\" width=\"300\" height=\"100\">\n",
    "\n",
    "**Excersie:** \n",
    "How many different movies are contained in `movie_genre_df`?"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": null,
   "metadata": {},
   "outputs": [],
   "source": [
    "movie_genre_df=pd.read_csv('movie_genre.csv')\n",
    "movie_genre_df.head()"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": null,
   "metadata": {},
   "outputs": [],
   "source": [
    "movie_genre_df_________.nunique()"
   ]
  },
  {
   "cell_type": "markdown",
   "metadata": {},
   "source": [
    "**Excersie:** Get the rows in `movie_genre_df` which have a name equal to Toy Story and save this as `toy_story_genres`."
   ]
  },
  {
   "cell_type": "code",
   "execution_count": null,
   "metadata": {},
   "outputs": [],
   "source": [
    "# Select only the rows with values in the name column equal to Toy Story\n",
    "toy_story_genres = movie_genre_df[_______[_______] == 'Toy Story']\n",
    "\n",
    "# Inspect the subset\n",
    "print(toy_story_genres)"
   ]
  },
  {
   "cell_type": "markdown",
   "metadata": {},
   "source": [
    "**Excersie:** Transform `movie_genre_df` to a table called `movie_cross_table`.\n",
    "\n",
    "Assign the subset of `movie_cross_table` that contains Toy Story to the variable `toy_story_genres_ct` and inspect the results."
   ]
  },
  {
   "cell_type": "code",
   "execution_count": null,
   "metadata": {},
   "outputs": [],
   "source": [
    "# Create cross-tabulated DataFrame from name and genre_list columns\n",
    "movie_cross_table = pd.____(movie_genre_df['name'], ____)\n",
    "\n",
    "# Select only the rows with Toy Story as the index\n",
    "toy_story_genres_ct = movie_cross_table[movie_cross_table.____ == 'Toy Story']\n",
    "print(toy_story_genres_ct)"
   ]
  },
  {
   "cell_type": "markdown",
   "metadata": {},
   "source": [
    "**Excersie:** Inspect the rows corresponding to 'Toy Story' and 'Jumanji' in movie_cross_table. How many genres do they have in common?"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": null,
   "metadata": {},
   "outputs": [],
   "source": []
  },
  {
   "cell_type": "markdown",
   "metadata": {},
   "source": [
    "## Making content-based recommendations\n",
    "Jaccard similarity:\n",
    "\n",
    "The Jaccard similarity is the ratio of attributes that two items have in common, divided by the total number of their combined attributes. These are respectively shown by the two orange shaded areas in the Venn diagrams here. It will always be between 0 and 1 and the more attributes the two items have in common, the higher the score.\n",
    "<img src=\"image/27.PNG\" width=\"500\" height=\"100\">\n",
    "\n",
    "\n",
    "`genres_array_df` :\n",
    "<img src=\"image/28.PNG\" width=\"500\" height=\"100\">\n",
    "\n",
    "### Calculating Jaccard similarity between books\n",
    "```python\n",
    "from sklearn.metrics import jaccard_score\n",
    "hobbit_row = book_genre_df.loc['The Hobbit']\n",
    "GOT_row = book_genre_df.loc['A Game of Thrones']\n",
    "print(jaccard_score(hobbit_row, GOT_row))\n",
    "```\n",
    "answer: `0.5`\n",
    "\n",
    "### Finding the distance between all items\n",
    "```python\n",
    "from scipy.spatial.distance import pdist, squareform\n",
    "jaccard_distances = pdist(book_genre_df.values, metric='jaccard')\n",
    "print(jaccard_distances)\n",
    "```\n",
    "answer:`[1. 0.5 1. 1. 0.5 1. ]`\n",
    "\n",
    "```python\n",
    "square_jaccard_distances = squareform(jaccard_distances)\n",
    "print(square_jaccard_distances)\n",
    "```\n",
    "answer:\n",
    "```python\n",
    "[[0. 1. 0.5 1. ]\n",
    "[1. 0. 1. 0.5]\n",
    "[0.5 1. 0. 1. ]\n",
    "[1. 0.5 1. 0. ]]```\n",
    "\n",
    "```python\n",
    "jaccard_similarity_array = 1 - square_jaccard_distances\n",
    "print(jaccard_similarity_array)\n",
    "\n",
    "answer:\n",
    "[[1. 0. 0.5 0. ]\n",
    "[0. 1. 0. 0.5]\n",
    "[0.5 0. 1. 0. ]\n",
    "[0. 0.5 0. 1. ]]\n",
    "```\n",
    "### Creating a usable distance table\n",
    "```python\n",
    "distance_df = pd.DataFrame(jaccard_similarity_array,\n",
    "index=genres_array_df['Book'],\n",
    "columns=genres_array_df['Book'])\n",
    "distance_df.head()\n",
    "```\n",
    "<img src=\"image/29.PNG\" width=\"500\" height=\"100\">\n",
    "\n",
    "\n",
    "```python\n",
    "print(distance_df['The Hobbit']['A Game of Thrones'])\n",
    "```\n",
    "answer: `0.75`\n",
    "\n",
    "```python\n",
    "print(distance_df['The Hobbit']['The Great Gatsby'])\n",
    "```\n",
    "answer: `0.15`\n",
    "\n",
    "### Finding the most similar books\n",
    "```python\n",
    "print(distance_df['The Hobbit'].sort_values(ascending=False))\n",
    "```\n",
    "<img src=\"image/30.PNG\" width=\"500\" height=\"100\">\n",
    "\n",
    "<img src=\"https://i1.wp.com/www.firstplaceforhealth.com/wp-content/uploads/2020/03/train-your-brain-gif-2.gif?fit=1200%2C904&ssl=1\" width=\"300\" height=\"100\">\n",
    "\n",
    "**Exercise:** Compare the movie `GoldenEye` with the movie `Toy Story`, and `GoldenEye` with `Cutthroat Island` and compare the results."
   ]
  },
  {
   "cell_type": "code",
   "execution_count": null,
   "metadata": {},
   "outputs": [],
   "source": [
    "# Import numpy and the Jaccard similarity function\n",
    "import numpy as np\n",
    "from sklearn.metrics import ______\n",
    "\n",
    "# Extract just the rows containing GoldenEye and Toy Story\n",
    "goldeneye_values = ____.____['GoldenEye'].____\n",
    "toy_story_values = ____.____['Toy Story'].____\n",
    "\n",
    "# Find the similarity between GoldenEye and Toy Story\n",
    "print(jaccard_score(goldeneye_values, toy_story_values))\n",
    "\n"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": null,
   "metadata": {},
   "outputs": [],
   "source": [
    "# Repeat for GoldenEye and Skyfall\n",
    "cutthroat_Island_values = movie_cross_table.____['Cutthroat Island'].____\n",
    "print(jaccard_score(goldeneye_values, cutthroat_Island_values))"
   ]
  },
  {
   "cell_type": "markdown",
   "metadata": {},
   "source": [
    "**Exercise**: Find the similarities between all movies and store them in a DataFrame for quick and easy lookup."
   ]
  },
  {
   "cell_type": "code",
   "execution_count": null,
   "metadata": {},
   "outputs": [],
   "source": [
    "# Import functions from scipy\n",
    "from scipy.spatial.distance import pdist, squareform\n",
    "\n",
    "# Calculate all pairwise distances\n",
    "jaccard_distances = ____(movie_cross_table.values, metric='____')\n",
    "\n",
    "# Convert the distances to a square matrix\n",
    "jaccard_similarity_array = 1 - ____(____)\n",
    "\n",
    "# Wrap the array in a pandas DataFrame\n",
    "jaccard_similarity_df = pd.____(jaccard_similarity_array, ____=movie_cross_table.index, ____=movie_cross_table.index)\n",
    "\n",
    "# Print the top 5 rows of the DataFrame\n",
    "print(jaccard_similarity_df.head())"
   ]
  },
  {
   "cell_type": "markdown",
   "metadata": {},
   "source": [
    "**Exercise:** Use this new DataFrame to suggest a movie recommendation."
   ]
  },
  {
   "cell_type": "code",
   "execution_count": null,
   "metadata": {},
   "outputs": [],
   "source": [
    "# Wrap the preloaded array in a DataFrame\n",
    "jaccard_similarity_df = pd.DataFrame(jaccard_similarity_array, index=movie_cross_table.index, columns=movie_cross_table.index)\n",
    "\n",
    "# Find the values for the movie Thor\n",
    "jaccard_similarity_series = jaccard_similarity_df.loc['Thor']\n",
    "\n",
    "# Sort these values from highest to lowest\n",
    "ordered_similarities = jaccard_similarity_series._____(ascending=False)\n",
    "\n",
    "# Print the results\n",
    "print(ordered_similarities)"
   ]
  },
  {
   "cell_type": "markdown",
   "metadata": {},
   "source": [
    "**Question**\n",
    "Based on your analysis, which movie in the dataset is most similar to Thor?"
   ]
  },
  {
   "cell_type": "markdown",
   "metadata": {},
   "source": [
    "## Text-based similarities\n",
    "\n",
    "### Working without clear attributes\n",
    "\n",
    "Unfortunately in the real world, this is often not the case as attribute labels such as book genres might not be available. Thankfully if there is text tied to an item then we may still be in luck. This could be a plot summary, an item description, or even the contents of a book itself. For this kind of data, we use \"Term Frequency Inverse Document Frequency\" or TF-IDF to transform the text into something usable.\n",
    "\n",
    "<img src=\"image/31.PNG\" width=\"500\" height=\"100\">\n",
    "\n",
    "### Term frequency inverse document frequency\n",
    "\n",
    "TF-IDF divides the number of times a word occurs in a document by a measure of what proportion of all the documents a word occurs in. This has the effect of reducing the value of common words while increasing the weight of words that do not occur in many documents.\n",
    "<img src=\"image/32.PNG\" width=\"500\" height=\"100\">\n",
    "\n",
    "`book_summary_df` :\n",
    "<img src=\"image/33.PNG\" width=\"500\" height=\"100\">\n",
    "\n",
    "### Instantiate the vectorizer:\n",
    "```python\n",
    "from sklearn.feature_extraction.text import TfidfVectorizer\n",
    "tfidfvec = TfidfVectorizer(min_df=2, max_df=0.7)\n",
    "```\n",
    "### Vectorizing the data\n",
    "```python\n",
    "vectorized_data = tfidfvec.fit_transform(book_summary_df['Descriptions'])\n",
    "print(tfidfvec.get_feature_names)\n",
    "```\n",
    "Answer:\n",
    "\n",
    "`['age', 'ancient', 'angry', 'brave', 'battle', 'fellow', 'game', 'general', ...]`\n",
    "```python\n",
    "print(vectorized_data.to_array())\n",
    "\n",
    "Answer:\n",
    "[[0.21, 0.53, 0.41, 0.64, 0.01, 0.02, ...\n",
    "[0.31, 0.00, 0.42, 0.03, 0.00, 0.73, ...\n",
    "[..., ..., ..., ..., ..., ..., ...\n",
    "```\n",
    "\n",
    "### Formatting the data\n",
    "```python\n",
    "tfidf_df = pd.DataFrame(vectorized_data.toarray(),\n",
    "columns=tfidfvec.get_feature_names())\n",
    "tfidf_df.index = book_summary_df['Book']\n",
    "print(tfidf_df)\n",
    "```\n",
    "<img src=\"image/34.PNG\" width=\"500\" height=\"100\">\n",
    "\n",
    "### Cosine similarity\n",
    "\n",
    "As we advance from Boolean features to continuous TF-IDF values, we will use a metric that's better at measuring between items that have more variation in their data; cosine similarity. We won't go into it in depth here, but mathematically, it's the measure of the angle between two documents in the high dimensional metric space as seen on this two-dimensional example. All values are between 0 and 1 where 1 is an exact match.\n",
    "\n",
    "<img src=\"image/35.PNG\" width=\"300\" height=\"100\">\n",
    "\n",
    "```python\n",
    "from sklearn.metrics.pairwise import cosine_similarity\n",
    "# Find similarity between all items\n",
    "cosine_similarity_array = cosine_similarity(tfidf_summary_df)\n",
    "# Find similarity between two items\n",
    "cosine_similarity(tfidf_df.loc['The Hobbit'].values.reshape(1, -1),\n",
    "tfidf_df.loc['Macbeth'].values.reshape(1, -1))\n",
    "```\n",
    "\n",
    "<img src=\"https://i1.wp.com/www.firstplaceforhealth.com/wp-content/uploads/2020/03/train-your-brain-gif-2.gif?fit=1200%2C904&ssl=1\" width=\"300\" height=\"100\">\n",
    "\n",
    "**Exercise:** Work with the `df_plots` DataFrame. It contains movies' names in the Title column and their plots in the Plot column."
   ]
  },
  {
   "cell_type": "code",
   "execution_count": null,
   "metadata": {},
   "outputs": [],
   "source": [
    "df_plots=pd.read_excel('df_plots.xlsx')\n",
    "df_plots"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": null,
   "metadata": {},
   "outputs": [],
   "source": [
    "from sklearn.feature_extraction.text import _________\n",
    "\n",
    "# Instantiate the vectorizer object to the vectorizer variable\n",
    "vectorizer = _____()\n",
    "\n",
    "# Fit and transform the plot column\n",
    "vectorized_data = vectorizer.______(df_plots['Plot'])\n",
    "\n",
    "# Look at the features generated\n",
    "print(vectorizer.__________())"
   ]
  },
  {
   "cell_type": "markdown",
   "metadata": {},
   "source": [
    "**Exercise:** Repeat the creation of the TfidfVectorizer, but this time, set the minimum document frequency to `2` and the maximum document frequency to `0.7`."
   ]
  },
  {
   "cell_type": "code",
   "execution_count": null,
   "metadata": {},
   "outputs": [],
   "source": [
    "from sklearn.feature_extraction.text import TfidfVectorizer\n",
    "\n",
    "# Instantiate the vectorizer object to the vectorizer variable\n",
    "vectorizer = TfidfVectorizer(________, ________)\n",
    "\n",
    "# Fit and transform the plot column\n",
    "vectorized_data = vectorizer.fit_transform(df_plots['Plot'])\n",
    "\n",
    "# Look at the features generated\n",
    "print(vectorizer.get_feature_names())"
   ]
  },
  {
   "cell_type": "markdown",
   "metadata": {},
   "source": [
    "**Exercise:** Create Dataframe from TF-IDFarray.  Assign the movie titles to the index and inspect"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": null,
   "metadata": {},
   "outputs": [],
   "source": [
    "from sklearn.feature_extraction.text import TfidfVectorizer\n",
    "\n",
    "# Instantiate the vectorizer object and transform the plot column\n",
    "vectorizer = ____(max_df=0.7, min_df=2)\n",
    "vectorized_data = vectorizer.____(df_plots['Plot']) \n",
    "\n",
    "# Create Dataframe from TF-IDFarray\n",
    "tfidf_df = pd.____(____.toarray(), columns=vectorizer.____())\n",
    "\n",
    "# Assign the movie titles to the index and inspect\n",
    "tfidf_df.____ = ____['Title']\n",
    "print(tfidf_df.head())"
   ]
  },
  {
   "cell_type": "markdown",
   "metadata": {},
   "source": [
    "**Exercise:** Generate a matrix of all of the movie cosine similarities and store them in a DataFrame for ease of lookup. This will allow you to compare movies and find recommendations quickly and easily."
   ]
  },
  {
   "cell_type": "code",
   "execution_count": null,
   "metadata": {},
   "outputs": [],
   "source": [
    "# Import cosine_similarity measure\n",
    "from sklearn.metrics.pairwise import ____\n",
    "\n",
    "# Create the array of cosine similarity values\n",
    "cosine_similarity_array = ____(tfidf_summary_df)\n",
    "\n",
    "# Wrap the array in a pandas DataFrame\n",
    "cosine_similarity_df = pd.____(cosine_similarity_array, ____=____.____, ____=____.____)\n",
    "\n",
    "# Print the top 5 rows of the DataFrame\n",
    "print(cosine_similarity_df.head())"
   ]
  },
  {
   "cell_type": "markdown",
   "metadata": {},
   "source": [
    "**Exercise:**  we pre-calculated the similarity ratings between all movies in the dataset based on their plots transformed by TF-IDF. Now put these similarity ratings in a DataFrame for ease of use. Then use this new DataFrame to suggest a movie recommendation."
   ]
  },
  {
   "cell_type": "code",
   "execution_count": null,
   "metadata": {},
   "outputs": [],
   "source": [
    "# Find the values for the movie Rio\n",
    "cosine_similarity_series = ____.____['Skyfall']\n",
    "\n",
    "# Sort these values highest to lowest\n",
    "ordered_similarities = cosine_similarity_series.____(____)\n",
    "\n",
    "# Print the results\n",
    "print(____)"
   ]
  },
  {
   "cell_type": "markdown",
   "metadata": {},
   "source": [
    "**Question**\n",
    "Based on your analysis, which movie in the dataset is most similar to Rio?"
   ]
  },
  {
   "cell_type": "markdown",
   "metadata": {},
   "source": [
    "## User profile recommendations\n",
    "### Item to item recommendations\n",
    "\n",
    "This has many uses such as suggesting obscure books that are similar to your favorite, proposing the next movie to watch that is like the one you just finished, or even finding alternative options when items are out of stock.\n",
    "<img src=\"image/36.PNG\" width=\"500\" height=\"100\">\n",
    "\n",
    "`tfidf_summary_df` :\n",
    "<img src=\"image/37.PNG\" width=\"500\" height=\"100\">\n",
    "\n",
    "### Extract the user data\n",
    "```python\n",
    "list_of_books_read = ['The Hobbit', 'Foundation', 'Nudge']\n",
    "user_books = tfidf_summary_df.reindex(list_of_books_read)\n",
    "print(user_books)\n",
    "```\n",
    "<img src=\"image/38.PNG\" width=\"500\" height=\"100\">\n",
    "\n",
    "### Build the user profile\n",
    "```python\n",
    "user_prof = user_books.mean()\n",
    "print(user_prof)\n",
    "```\n",
    "<img src=\"image/39.PNG\" width=\"500\" height=\"100\">\n",
    "\n",
    "```python\n",
    "print(user_prof.values.reshape(1,-1))\n",
    "```\n",
    "answer:\n",
    "`[0.376667, .480000, 0.426667, 0.256667, ...]`\n",
    "\n",
    "### Finding recommendations for a user\n",
    "```python\n",
    "# Create a subset of only the non read books\n",
    "non_user_books = tfidf_summary_df.drop(list_of_books_read, axis=0)\n",
    "# Calculate the cosine similarity between all rows\n",
    "user_prof_similarities = cosine_similarity(user_prof.values.reshape(1, -1),\n",
    "non_user_books)\n",
    "# Wrap in a DataFrame for ease of use\n",
    "user_prof_similarities_df = pd.DataFrame(user_prof_similarities.T,\n",
    "index=tfidf_summary_df.index,\n",
    "columns=[\"similarity_score\"])\n",
    "```\n",
    "### Getting the top recommendations\n",
    "```python\n",
    "sorted_similarity_df = user_prof_similarities.sort_values(by=\"similarity_score\",\n",
    "ascending=False)\n",
    "print(sorted_similarity_df)\n",
    "```\n",
    "<img src=\"image/40.PNG\" width=\"500\" height=\"100\">\n",
    "\n",
    "<img src=\"https://i1.wp.com/www.firstplaceforhealth.com/wp-content/uploads/2020/03/train-your-brain-gif-2.gif?fit=1200%2C904&ssl=1\" width=\"300\" height=\"100\">\n",
    "\n",
    "**Exercise:** Work through how one could create recommendations based on a user and all the items they liked as opposed to a singular item. You will first generate a profile for a user by aggregating all of the movies they have previously enjoyed."
   ]
  },
  {
   "cell_type": "code",
   "execution_count": null,
   "metadata": {},
   "outputs": [],
   "source": [
    "list_of_movies_enjoyed = ['Skyfall', 'Jumanji', 'Sudden Death']\n",
    "\n",
    "# Create a subset of only the movies the user has enjoyed\n",
    "movies_enjoyed_df = tfidf_summary_df.____(____)\n",
    "\n",
    "# Inspect the DataFrame\n",
    "print(movies_enjoyed_df)"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": null,
   "metadata": {},
   "outputs": [],
   "source": [
    "# Generate the user profile by finding the average scores of movies they enjoyed\n",
    "user_prof = ____.____()\n",
    "\n",
    "# Inspect the results\n",
    "print(____)"
   ]
  },
  {
   "cell_type": "markdown",
   "metadata": {},
   "source": [
    "**Exercise:** Find the subset of `tfidf_df` that does not include movies in `list_of_movies_enjoyed` and assign it to `tfidf_subset_df`."
   ]
  },
  {
   "cell_type": "code",
   "execution_count": null,
   "metadata": {},
   "outputs": [],
   "source": [
    "from sklearn.metrics.pairwise import cosine_similarity\n",
    "\n",
    "# Find subset of tfidf_df that does not include movies in list_of_movies_enjoyed\n",
    "tfidf_subset_df = tfidf_df.____(list_of_movies_enjoyed, axis=____)"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": null,
   "metadata": {},
   "outputs": [],
   "source": [
    "# Calculate the cosine_similarity and wrap it in a DataFrame\n",
    "similarity_array = ____(user_prof.values.reshape(1, -1), ____)\n",
    "similarity_df = pd.____(similarity_array.T, ____=tfidf_subset_df.index, columns=[\"similarity_score\"])"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": null,
   "metadata": {},
   "outputs": [],
   "source": [
    "# Sort the values from high to low by the values in the similarity_score\n",
    "sorted_similarity_df = similarity_df.____(by=\"similarity_score\", ____=____)\n",
    "\n",
    "# Inspect the most similar to the user preferences\n",
    "print(____.____())"
   ]
  },
  {
   "cell_type": "markdown",
   "metadata": {},
   "source": [
    "<img src=\"image/c3.PNG\" width=\"1000\" height=\"100\">\n"
   ]
  },
  {
   "cell_type": "markdown",
   "metadata": {},
   "source": [
    "## Collaborative filtering\n",
    "Collaborative filtering is the name given to the prediction, or filtering, of items that might interest a user based on the preferences of similar users. It works around the premise that person A has similar tastes to person B and C.\n",
    "and both person B and C also like a certain item,\n",
    "then it is likely that person A would also like that new item.\n",
    "\n",
    "<img src=\"image/43.PNG\" width=\"500\" height=\"100\">\n",
    "\n",
    "### Finding similar users\n",
    "\n",
    "<img src=\"image/45.PNG\" width=\"500\" height=\"100\">\n",
    "\n",
    "### Working with real data\n",
    "`user_ratings` DataFrame:\n",
    "<img src=\"image/46.PNG\" width=\"500\" height=\"100\">\n",
    "\n",
    "### Pivoting our data\n",
    "\n",
    "```python\n",
    "user_ratings_pivot = user_ratings.pivot(index='User',\n",
    "columns='Book',\n",
    "values='Rating')\n",
    "print(user_ratings_pivot)\n",
    "```\n",
    "<img src=\"image/47.PNG\" width=\"500\" height=\"100\">\n",
    "\n",
    "### Data sparsity\n",
    "\n",
    "```python\n",
    "print(user_ratings_pivot.dropna())\n",
    "```\n",
    "<img src=\"image/48.PNG\" width=\"500\" height=\"100\">\n",
    "\n",
    "### Filling the missing values\n",
    "For example the second user here. They loved Catcher in the Rye, and enjoyed Fifty Shades of Grey, but have not rated The Great Gatsby. If we were to fill this NaN with a 0, we would be incorrectly implying they greatly disliked the book compared to the others, which we can't say for sure.\n",
    "```python\n",
    "print(user_ratings_pivot[\"User_651\"].fillna(0))\n",
    "```\n",
    "<img src=\"image/49.PNG\" width=\"500\" height=\"100\">\n",
    "\n",
    "\n",
    "One alternative is to center each user's ratings around 0 by deducting the row average and then fill in the missing values with 0. This means the missing data is replaced with neutral scores.\n",
    "<img src=\"image/50.PNG\" width=\"500\" height=\"100\">\n",
    "\n",
    "\n",
    "```python\n",
    "avg_ratings = user_ratings_pivot.mean(axis=1)\n",
    "user_ratings_pivot = user_ratings_pivot.sub(avg_ratings, axis=0)\n",
    "print(user_ratings_pivot)\n",
    "```\n",
    "<img src=\"image/51.PNG\" width=\"500\" height=\"100\">\n",
    "\n",
    "```python\n",
    "user_ratings_pivot.fillna(0)\n",
    "```\n",
    "<img src=\"image/52.PNG\" width=\"500\" height=\"100\">\n",
    "\n",
    "\n",
    "<img src=\"https://i1.wp.com/www.firstplaceforhealth.com/wp-content/uploads/2020/03/train-your-brain-gif-2.gif?fit=1200%2C904&ssl=1\" width=\"300\" height=\"100\">\n",
    "**Exercise:** Compare movies and see whether they have received similar reviewing patterns."
   ]
  },
  {
   "cell_type": "code",
   "execution_count": null,
   "metadata": {},
   "outputs": [],
   "source": [
    "user_ratings=pd.read_csv('user_ratings1.csv')\n",
    "user_ratings"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": null,
   "metadata": {},
   "outputs": [],
   "source": [
    "user_ratings_df = user_ratings.pivot(index='userId',\n",
    "columns='title',\n",
    "values='rating')\n",
    "user_ratings_df"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": null,
   "metadata": {},
   "outputs": [],
   "source": [
    "from sklearn.impute import SimpleImputer\n",
    "imp = SimpleImputer(missing_values=np.nan, strategy='mean')\n",
    "user_ratings_df_imputed=imp.fit_transform(user_ratings_df)\n",
    "user_ratings_df_imputed=pd.DataFrame(user_ratings_df_imputed,index=user_ratings_df.index, columns=user_ratings_df.columns)"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": null,
   "metadata": {},
   "outputs": [],
   "source": [
    "from sklearn.metrics.pairwise import cosine_similarity\n",
    "\n",
    "# Assign the arrays to variables\n",
    "sw_IV = movie_ratings_centered.loc[:,'Toy Story (1995)'].values.reshape(1, -1)\n",
    "sw_V = ____.____[:,'Bambi (1942)'].____.____(1, -1)\n",
    "\n",
    "# Find the similarity between two Star Wars movies\n",
    "similarity_A = cosine_similarity(____, ____)\n",
    "print(similarity_A)"
   ]
  },
  {
   "cell_type": "markdown",
   "metadata": {},
   "source": [
    "## Finding similarities\n",
    "\n",
    "### Item-based recommendations\n",
    "It assumes if Item A and B receive similar reviews, either positive or negative,\n",
    "Then however other people feel about A, they should feel the same way about B.\n",
    "\n",
    "<img src=\"image/53.PNG\" width=\"500\" height=\"100\">\n",
    "\n",
    "### User-based to item-based\n",
    "We can switch between these two approaches,by transposing the matrices giving us the items as rows and the users as columns.\n",
    "\n",
    "<img src=\"image/55.PNG\" width=\"500\" height=\"100\">\n",
    "\n",
    "```python\n",
    "print(user_ratings_pivot)\n",
    "```\n",
    "<img src=\"image/56.PNG\" width=\"500\" height=\"100\">\n",
    "\n",
    "```python\n",
    "book_ratings_pivot = user_ratings_pivot.T\n",
    "print(book_ratings_pivot)\n",
    "```\n",
    "<img src=\"image/57.PNG\" width=\"500\" height=\"100\">\n",
    "\n",
    "\n",
    "### Cosine similarities\n",
    "```python\n",
    "cosine_similarity(book_ratings_pivot.loc['Lord of the Rings', :].values.reshape(1, -1),\n",
    "book_ratings_pivot.loc['The Hobbit', :].values.reshape(1, -1))\n",
    "```\n",
    "answer:\n",
    "`0.43`\n",
    "\n",
    "```python\n",
    "cosine_similarity(book_ratngs.loc['Lord of the Rings', :].values.reshape(1, -1),\n",
    "book_ratngs.loc['Twilight', :].values.reshape(1, -1))\n",
    "```\n",
    "answer:\n",
    "`-0.64`\n",
    "\n",
    "```python\n",
    "similarities = cosine_similarity(book_ratings_pivot)\n",
    "cosine_similarity_df = pd.DataFrame(similarities,\n",
    "index=book_ratings_pivot.index,\n",
    "columns=book_ratings_pivot.index)\n",
    "cosine_similarity_df.head()\n",
    "```\n",
    "<img src=\"image/58.PNG\" width=\"500\" height=\"100\">\n",
    "\n",
    "```python\n",
    "cosine_similarity_series = cosine_similarity_df.loc['The Hobbit']\n",
    "ordered_similarities = cosine_similarity_series.sort_values(ascending=False)\n",
    "print(ordered_similarities)\n",
    "```\n",
    "<img src=\"image/59.PNG\" width=\"500\" height=\"100\">\n",
    "\n",
    "<img src=\"https://i1.wp.com/www.firstplaceforhealth.com/wp-content/uploads/2020/03/train-your-brain-gif-2.gif?fit=1200%2C904&ssl=1\" width=\"300\" height=\"100\">\n",
    "\n",
    "**Exercise:** Switch between the user-based and item-based and compare their values."
   ]
  },
  {
   "cell_type": "code",
   "execution_count": null,
   "metadata": {},
   "outputs": [],
   "source": [
    "user_ratings_subset2=pd.read_csv('user_ratings_subset2.csv')\n",
    "user_ratings_subset2"
   ]
  },
  {
   "cell_type": "markdown",
   "metadata": {},
   "source": [
    "**Question**\n",
    "\n",
    "Based on the data in `user_ratings_subset2`, which user is most similar to `User_A`?"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": null,
   "metadata": {},
   "outputs": [],
   "source": [
    "# Transpose the user_ratings_subset DataFrame\n",
    "movie_ratings_subset = ____.____\n",
    "\n",
    "print(movie_ratings_subset)"
   ]
  },
  {
   "cell_type": "markdown",
   "metadata": {},
   "source": [
    "**Question**\n",
    "\n",
    "Based on this new transposed data, what movie appears most similar to `The Sandlot`?"
   ]
  },
  {
   "cell_type": "markdown",
   "metadata": {},
   "source": [
    "## Using K-nearest neighbors\n",
    "\n",
    "### Beyond similar items\n",
    "You are now able to find similar items based on how the users in your dataset have rated them. But what if we wanted to not only find similarly rated items, but actually predict how a user might rate an item even if it is not similar to any item they have seen! One approach is to find similar users using a K nearest neighbors model and see how they liked the item.\n",
    "\n",
    "<img src=\"image/60.PNG\" width=\"500\" height=\"100\">\n",
    "\n",
    "### K-nearest neighbors\n",
    "K-NN finds the k users that are closest measured by a specified metric, to the user in question. It then averages the rating those users gave the item we are trying to get a rating for.\n",
    "<img src=\"image/61.PNG\" width=\"500\" height=\"100\">\n",
    "\n",
    "### User-user similarity\n",
    "```python\n",
    "similarities = cosine_similarity(user_ratings_pivot)\n",
    "cosine_similarity_df = pd.DataFrame(user_ratings_pivot,\n",
    "index=user_ratings_pivot.index,\n",
    "columns=user_ratings_pivot.index)\n",
    "cosine_similarity_df.head()\n",
    "```\n",
    "<img src=\"image/62.PNG\" width=\"500\" height=\"100\">\n",
    "\n",
    "### Step by step KNN\n",
    "\n",
    "```python\n",
    "user_similarity_series = user_similarities.loc['user_001']\n",
    "ordered_similarities = user_similarity_series.sort_values(ascending=False)\n",
    "nearest_neighbors = ordered_similarities[1:4].index\n",
    "print(nearest_neighbors)\n",
    "```\n",
    "answer:\n",
    "\n",
    "`user_007`\n",
    "`user_042`\n",
    "`user_003`\n",
    "\n",
    "```python\n",
    "neighbor_ratings = user_ratings_table.reindex(nearest_neighbors)\n",
    "neighbor_ratings['Catch-22'].mean()\n",
    "```\n",
    "answer:\n",
    "`3.2`\n",
    "\n",
    "### Using scikit-learn's KNN\n",
    "```python\n",
    "print(user_ratings_pivot)\n",
    "```\n",
    "<img src=\"image/63.PNG\" width=\"500\" height=\"100\">\n",
    "\n",
    "```python\n",
    "print(user_ratings_table)\n",
    "```\n",
    "<img src=\"image/64.PNG\" width=\"500\" height=\"100\">\n",
    "\n",
    "```python\n",
    "user_ratings_pivot.drop(\"Catch-22\", axis=1, inplace=True)\n",
    "target_user_x = user_ratings_pivot.loc[[\"user_001\"]]\n",
    "print(target_user_x)\n",
    "```\n",
    "<img src=\"image/65.PNG\" width=\"500\" height=\"100\">\n",
    "\n",
    "```python\n",
    "other_users_y = user_ratings_table[\"Catch-22\"]\n",
    "print(other_users_y)\n",
    "```\n",
    "answer:\n",
    "`[NaN, '5.0', '3.0', '4.0', '5.0' ...]`\n",
    "\n",
    "\n",
    "```python\n",
    "other_users_x = user_ratings_pivot[other_users_y.notnull()]\n",
    "print(other_users_x)\n",
    "```\n",
    "<img src=\"image/66.PNG\" width=\"500\" height=\"100\">\n",
    "\n",
    "```python\n",
    "other_users_y.dropna(inplace=True)\n",
    "print(other_users_y)\n",
    "```\n",
    "answer:\n",
    "`['5.0', '3.0', '4.0','5.0' ...]`\n",
    "\n",
    "```python\n",
    "from sklearn.neighbors import KNeighborsRegressor\n",
    "user_knn = KNeighborsRegressor(metric='cosine', n_neighbors=3)\n",
    "user_knn.fit(other_users_x, other_users_y)\n",
    "user_user_pred = user_knn.predict(target_user_x)\n",
    "print(user_user_pred)\n",
    "```\n",
    "answer:\n",
    "`3.3`\n",
    "\n",
    "```python\n",
    "from sklearn.neighbors import KNeighborsClassifier\n",
    "user_knn = KNeighborsClassifier(metric='cosine', n_neighbors=3)\n",
    "user_knn.fit(other_users_x, other_users_y)\n",
    "user_user_pred = user_knn.predict(target_user_x)\n",
    "print(user_user_pred)\n",
    "```\n",
    "answer:\n",
    "`3`\n",
    "\n",
    "<img src=\"https://i1.wp.com/www.firstplaceforhealth.com/wp-content/uploads/2020/03/train-your-brain-gif-2.gif?fit=1200%2C904&ssl=1\" width=\"300\" height=\"100\">\n"
   ]
  },
  {
   "cell_type": "markdown",
   "metadata": {},
   "source": [
    "## Item-based or user-based\n",
    "### Item-based filtering\n",
    "\n",
    "<img src=\"image/67.PNG\" width=\"500\" height=\"100\">\n",
    "<img src=\"image/68.PNG\" width=\"500\" height=\"100\">\n",
    "\n",
    "### Why use item-based filtering?\n",
    "#### Pros:\n",
    "- Item-based recommendations are more consistent over time\n",
    "\n",
    "        Users' preferences change, for example, you might enjoy animated movies when you are younger, but change your preferences to action movies later in life. Items on the other hand do not usually change, a movie that was a horror movie when it came out is still a horror movie years later. \n",
    "        \n",
    "- Item-based recommendations can be easier to explain\n",
    "- Item-based recommendations can be pre-calculated\n",
    "#### Cons:\n",
    "- Item-based recommendations result in very obvious suggestions\n",
    "\n",
    "### Why use user-based filtering?\n",
    "#### Pros:\n",
    "- User-based recommendations can create a lot more interesting suggestions\n",
    "\n",
    "#### Cons:\n",
    "- Generally beaten by item-based recommendations using standard metrics\n",
    "\n",
    "<img src=\"https://i1.wp.com/www.firstplaceforhealth.com/wp-content/uploads/2020/03/train-your-brain-gif-2.gif?fit=1200%2C904&ssl=1\" width=\"300\" height=\"100\">\n",
    "\n"
   ]
  },
  {
   "cell_type": "markdown",
   "metadata": {},
   "source": [
    "<img src=\"image/c4.PNG\" width=\"1000\" height=\"100\">\n"
   ]
  },
  {
   "cell_type": "markdown",
   "metadata": {},
   "source": [
    "## Dealing with sparsity\n",
    "- Now you are capable of not only generating recommendations for any user in your dataset, but also predicting what rating users might give items they have not come across using KNN.\n",
    "- This works great for dense datasets in which every item has been reviewed by multiple people, which ensures that the K nearest neighbors are genuinely similar to your user. But what if the data is less full?\n",
    "- This is actually a common concern in real-world rating data as the number of users and items are generally quite high and the number of reviews are quite low.\n",
    "\n",
    "### Sparse matrices\n",
    "\n",
    "We call the percentage of a DataFrame that is empty the DataFrame's sparsity. In other words, the number of empty cells over the number of cells with data.\n",
    "\n",
    "<img src=\"image/69.PNG\" width=\"500\" height=\"100\">\n",
    "\n",
    "### Measuring sparsity\n",
    "We can check the sparsity of a DataFrame by counting the number of missing values in it using the following code. Here we see that the DataFrame is only just over 1% filled, so it's quite sparse.\n",
    "\n",
    "```python\n",
    "print(book_rating_df)\n",
    "```\n",
    "<img src=\"image/51.PNG\" width=\"500\" height=\"100\">\n",
    "\n",
    "```python\n",
    "number_of_empty = book_ratings_df.isnull().values.sum()\n",
    "total_number = user_ratings_df.size\n",
    "sparsity = number_of_empty/total_number\n",
    "print(sparsity)\n",
    "```\n",
    "Answer: \n",
    "`0.0114`\n",
    "### Why sparsity matters\n",
    "\n",
    "Why does this matter? This can create problems if we were to use KNN with sparse data because KNN requires you to find the K nearest users that have rated the item. \n",
    "<img src=\"image/70.PNG\" width=\"500\" height=\"100\">\n",
    "\n",
    "- Let's say we wanted to estimate what User 1 would give item 5. We would find the n nearest ratings of the item, but in this case, there are only 2 KNN or other users that have rated the item.\n",
    "- Therefore we would have to return an average of all reviews (2 in this case) because there is no other data. This does not actually take the similarities into account.\n",
    "\n",
    "\n",
    "### Measuring sparsity per column\n",
    "\n",
    "You can understand the scale of this issue by simply counting the number of actual reviews for each book using the following code. We can see that a large number of books have only received one or two reviews.\n",
    "```python\n",
    "user_ratings_df.notnull().sum()\n",
    "```\n",
    "<img src=\"image/71.PNG\" width=\"500\" height=\"100\">\n",
    "\n",
    "### Matrix factorization\n",
    "We can leverage matrix factorization to deal with this problem remarkably well and create some quite interesting features while doing so.\n",
    "\n",
    "Matrix factorization is when we decompose the user-rating matrix into the product of two lower dimensionality matrices. These matrices shown here are factors of the original matrix on the left, if you were to find the product of the two of them it would be this original matrix. By finding factors of the sparse matrix and then multiplying them together we can be left will a fully filled matrix. \n",
    "\n",
    "<img src=\"image/72.PNG\" width=\"500\" height=\"100\">\n",
    "\n",
    "### Matrix multiplication\n",
    "\n",
    "To multiply two rectangular matrices, the number of rows in the first matrix M here and the number of columns in the second matrix N here do not have to match but the number of columns of the first matrix must match the number of rows in the second.\n",
    "\n",
    "<img src=\"image/73.PNG\" width=\"500\" height=\"100\">\n",
    "\n",
    "```python\n",
    "print(matrix_x)\n",
    "\n",
    "Answer:\n",
    "    \n",
    "[[4, 1],\n",
    "[2, 2],\n",
    "[3, 3]]\n",
    "\n",
    "print(matrix_b)\n",
    "\n",
    "Answer:\n",
    "    \n",
    "[[1, 0, 4],\n",
    "[0, 1, 6]]\n",
    "\n",
    "\n",
    "import numpy as np\n",
    "dot_product = np.dot(matrix_x, matrix_b)\n",
    "print(dot_product)\n",
    "\n",
    "Answer:\n",
    "    \n",
    "[[ 4 1 22]\n",
    "[ 2 2 20]\n",
    "[ 3 3 30]]\n",
    "```\n",
    "<img src=\"https://i1.wp.com/www.firstplaceforhealth.com/wp-content/uploads/2020/03/train-your-brain-gif-2.gif?fit=1200%2C904&ssl=1\" width=\"300\" height=\"100\">\n",
    "\n",
    "\n",
    "**Exercise:** Calculate how sparse the movie_lens ratings data is by counting the number of occupied cells and compare it to the size of the full DataFrame. "
   ]
  },
  {
   "cell_type": "code",
   "execution_count": null,
   "metadata": {},
   "outputs": [],
   "source": [
    "# Count the occupied cells\n",
    "sparsity_count = user_ratings_df.____().____.____()\n",
    "\n",
    "# Count all cells\n",
    "full_count = user_ratings_df.____\n",
    "\n",
    "# Find the sparsity of the DataFrame\n",
    "sparsity = ____ / ____\n",
    "print(sparsity)"
   ]
  },
  {
   "cell_type": "markdown",
   "metadata": {},
   "source": [
    "**Exercise:** Count how often each movie in the user_ratings_df DataFrame has been given a rating, and then see how many have only one or two ratings."
   ]
  },
  {
   "cell_type": "code",
   "execution_count": null,
   "metadata": {},
   "outputs": [],
   "source": [
    "# Count the occupied cells per column\n",
    "occupied_count = user_ratings_df.____().____()\n",
    "print(occupied_count)"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": null,
   "metadata": {},
   "outputs": [],
   "source": [
    "# Sort the resulting series from low to high\n",
    "sorted_occupied_count = occupied_count.____()\n",
    "print(____)"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": null,
   "metadata": {},
   "outputs": [],
   "source": [
    "# Plot a histogram of the values in sorted_occupied_count\n",
    "sorted_occupied_count.____()\n",
    "plt.show()"
   ]
  },
  {
   "cell_type": "markdown",
   "metadata": {},
   "source": [
    "**Matrix multiplication**\n",
    "\n",
    "<img src=\"image/85.PNG\" width=\"500\" height=\"100\">\n",
    "\n",
    "Image of a tall rectangular matrix being multiplied by a wide rectangular matrix When multiplying an 10 x 30 matrix by a 30 x 40 matrix, what size matrix is generated?"
   ]
  },
  {
   "cell_type": "markdown",
   "metadata": {},
   "source": [
    "## Matrix factorization\n",
    "### Why this helps with sparse matrices\n",
    "\n",
    "A huge benefit of this, when performed in conjunction with recommendation systems, is that factors can be found as long as there is at least one value in every row and column. Or in other words every user has given at least one rating, and every item has been rated at least once. Why is this valuable? Because we can multiply these factors together to create a fully filled in matrix.\n",
    "<img src=\"image/72.PNG\" width=\"500\" height=\"100\">\n",
    "\n",
    "### What matrix factorization looks like\n",
    "\n",
    "Matrix factorization breaks a matrix into two component matrices. Take a rating matrix with M users as rows and the N items they rated as the columns. Matrix factorization will break this down into one matrix with its depth equal to the number of users and one matrix with its width equal to the number of items.\n",
    "The number of values in the newly created dimensions shown here are called the rank of the matrix and must be equal to each other and can be decided by us. These new unlabeled columns and rows are called latent features. These are the features that the matrix factorization view as mathematically the best ways to describe or sum up this dataset in the least number of features.\n",
    "\n",
    "<img src=\"image/74.PNG\" width=\"500\" height=\"100\">\n",
    "\n",
    "### Latent features\n",
    "To explain what that entails, let's take a closer look at a small example. Here we see four users and how they have rated six books and the decomposed version of the ratings matrix. You can see that the original matrix has six columns but the first matrix that is a factor only has two columns.\n",
    "\n",
    "Taking a look at latent feature 1, we can see that users who gave high ratings to horror and fantasy books got relatively high values for this feature, while for latent feature 2, a high value appears to correspond with users who preferred romance novels. This is a simplified example, and often latent features become harder to label with larger datasets, but these are features that the matrix factorization has calculated as representing patterns in the original matrix.\n",
    "\n",
    "\n",
    "<img src=\"image/75.PNG\" width=\"500\" height=\"100\">\n",
    "\n",
    "### Information loss\n",
    "One question that might come to mind when you see these large DataFrames being reduced to much smaller factor matrices is, how can it do this without losing information? In reality, you can't reduce down these matrices without at least some information loss - these factors are just close approximations of the original data. If we were to multiply the factors back together\n",
    "we would actually see a slight difference between the first and last matrix. Even the values we had originally may be off by a small fraction. \n",
    "<img src=\"image/76.PNG\" width=\"500\" height=\"100\">\n",
    "\n",
    "\n",
    "<img src=\"https://i1.wp.com/www.firstplaceforhealth.com/wp-content/uploads/2020/03/train-your-brain-gif-2.gif?fit=1200%2C904&ssl=1\" width=\"300\" height=\"100\">\n"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": null,
   "metadata": {},
   "outputs": [],
   "source": [
    "import numpy as np\n",
    "import pandas as pd\n",
    "user_matrix=np.array([[-3.9202913 ,  5.43628339],\n",
    "                       [ 2.01738539,  6.37262296],\n",
    "                       [ 1.46328822,  6.84850675],\n",
    "                       [-3.88683688,  5.47272837],\n",
    "                       [ 3.00873893,  6.54960159]])\n",
    "user_matrix"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": null,
   "metadata": {},
   "outputs": [],
   "source": [
    "original_df=pd.read_csv('original_df.csv')\n",
    "original_df"
   ]
  },
  {
   "cell_type": "markdown",
   "metadata": {},
   "source": [
    "Based on the values in the first column of the `user_matrix`, what do you think the latent feature may be summarizing?"
   ]
  },
  {
   "cell_type": "markdown",
   "metadata": {},
   "source": [
    "**Exercise:** Inspect the same original pre-factorization DataFrame from the last exercise loaded as `original_df`, and compare it to the product of its two factors, `user_matrix` and `item_matrix`."
   ]
  },
  {
   "cell_type": "code",
   "execution_count": null,
   "metadata": {},
   "outputs": [],
   "source": [
    "import numpy as np\n",
    "\n",
    "item_matrix=np.array([[-0.31315676,  0.27223577,  0.11236206, -0.6851852 ,  0.58797665],\n",
    "       [ 0.53506273, -0.67846698,  0.34679691, -0.33676863,  0.14038957],\n",
    "       [-0.58681644, -0.56753532,  0.01699735,  0.40147032,  0.41482863],\n",
    "       [ 0.34689509,  0.37669009,  0.48522268,  0.50094181,  0.50138271]])\n",
    "\n",
    "user_matrix=np.array([[-0.92214831,  0.46868881, -3.93546218,  6.20017019],\n",
    "       [ 0.29451291,  1.1337195 ,  2.00684562,  7.52205181],\n",
    "       [-0.23338272, -1.58223229,  1.42823577,  7.4428733 ],\n",
    "       [ 0.93640606, -0.2676827 , -3.90282275,  6.23040608],\n",
    "       [-0.07751554,  0.26188815,  2.99513239,  7.67609853]])\n",
    "\n",
    "# Multiply the user and item matrices\n",
    "predictions_df = ____.____(____, ____)\n",
    "# Inspect the recreated DataFrame\n",
    "print(predictions_df)\n",
    "\n",
    "# Inspect the original DataFrame and compare\n",
    "print(original_df)"
   ]
  },
  {
   "cell_type": "markdown",
   "metadata": {},
   "source": [
    "## Singular value decomposition (SVD)\n",
    "### What SVD does\n",
    "Singular value decomposition finds factors for your matrix.\n",
    "U is the user matrix V transpose is the features matrix (transpose in this case means that V has been flipped over its diagonal, but we do not need to worry about that here) but it also generates sigma as seen here, which is simply a diagonal matrix which can be thought of as the weights of the latent features, or how large an impact they are calculated to have.\n",
    "\n",
    "<img src=\"image/77.PNG\" width=\"500\" height=\"100\">\n",
    "\n",
    "### Prepping our data\n",
    "\n",
    "```python\n",
    "print(book_ratings_df.shape)\n",
    "```\n",
    "answer:\n",
    "`(220, 500)`\n",
    "\n",
    "```python\n",
    "avg_ratings = book_ratings_df.mean(axis=1)\n",
    "print(avg_ratings)\n",
    "\n",
    "answer:\n",
    "array([[4.5 ],\n",
    "[3.5],\n",
    "[2.5],\n",
    "[3.5],\n",
    "...\n",
    "[2.2]])\n",
    "```\n",
    "\n",
    "```python\n",
    "user_ratings_pivot_centered = user_ratings_df.sub(avg_ratings, axis=0)\n",
    "user_ratings_df.fillna(0, inplace=True)\n",
    "print(user_ratings_df)\n",
    "```\n",
    "<img src=\"image/52.PNG\" width=\"500\" height=\"100\">\n",
    "\n",
    "### Applying SVD\n",
    "```python\n",
    "from scipy.sparse.linalg import svds\n",
    "U, sigma, Vt = svds(user_ratings_pivot_centered)\n",
    "print(U.shape)\n",
    "```\n",
    "amswer:\n",
    "`(610, 6)`\n",
    "```python\n",
    "print(Vt.shape)\n",
    "```\n",
    "answer:\n",
    "`(6, 1000)`\n",
    "\n",
    "```python\n",
    "print(sigma)\n",
    "```\n",
    "`[3.0, 4.8, -12.6, -3.8, 8.2, 7.3]`\n",
    "\n",
    "```python\n",
    "sigma = np.diag(sigma)\n",
    "print(sigma)\n",
    "\n",
    "answer:\n",
    "array([ 3.0 , 0. , 0. , 0. , 0. , 0. ],\n",
    "[ 0. , 4.8 , 0. , 0. , 0. , 0. ],\n",
    "[ 0. , 0. , -12.6 , 0. , 0. , 0. ],\n",
    "[ 0. , 0. , 0. , -3.8 , 0. , 0. ],\n",
    "[ 0. , 0. , 0. , 0. , 8.2 , 0. ],\n",
    "[ 0. , 0. , 0. , 0. , 0. , 7.3 ]),\n",
    "```\n",
    "### Getting the final matrix\n",
    "<img src=\"image/78.PNG\" width=\"500\" height=\"100\">\n",
    "\n",
    "### Calculating the product in Python\n",
    "```python\n",
    "recalculated_ratings = np.dot(np.dot(U, sigma), Vt)\n",
    "print(recalculated_ratings)\n",
    "\n",
    "answer:\n",
    "[[ 0.1 -0.9 -3.6. ... ]\n",
    "[ -2.3 0.5 -0.5 ... ]\n",
    "[ 0.5 -0.5 2.0 ... ]\n",
    "[ ... ... ... ... ]]\n",
    "```\n",
    "### Add averages back\n",
    "```python\n",
    "recalculated_ratings = recalculated_ratings + avg_ratings.values.reshape(-1, 1)\n",
    "print(recalculated_ratings)\n",
    "\n",
    "answer:\n",
    "[[ 4.6 3.6 0.9 ... ]\n",
    "[ 1.8 4.0 3.0 ... ]\n",
    "[ 3.0 2.0 4.5 ... ]\n",
    "[ ... ... ... ... ]]\n",
    "\n",
    "print(book_ratings_df)\n",
    "\n",
    "answer:\n",
    "[[ 5.0 4.0 NA ... ]\n",
    "[ NA 4.0 3.0 ... ]\n",
    "[ 3.0 2.0 NA ... ]\n",
    "[ ... ... ... ... ]]\n",
    "```\n",
    "\n",
    "<img src=\"https://i1.wp.com/www.firstplaceforhealth.com/wp-content/uploads/2020/03/train-your-brain-gif-2.gif?fit=1200%2C904&ssl=1\" width=\"300\" height=\"100\">\n",
    "\n",
    "**Exercise:** Begin prepping the movie rating DataFrame you have been working with in order to be able to perform Singular value decomposition."
   ]
  },
  {
   "cell_type": "code",
   "execution_count": null,
   "metadata": {},
   "outputs": [],
   "source": [
    "# Get the average rating for each user \n",
    "avg_ratings = user_ratings_df.____(axis=1)\n",
    "\n",
    "# Center each user's ratings around 0\n",
    "user_ratings_centered = user_ratings_df.____(____, axis=1)\n",
    "\n",
    "# Fill in all missing values with 0s\n",
    "user_ratings_centered.____(0, inplace=True)\n",
    "\n",
    "# Print the mean of each column\n",
    "print(user_ratings_centered.____(axis=1))"
   ]
  },
  {
   "cell_type": "markdown",
   "metadata": {},
   "source": [
    "**Exercise:** Break the `user_ratings_centered` data you generated in the last exercise into 3 factors: `U`, `sigma`, and `Vt`."
   ]
  },
  {
   "cell_type": "code",
   "execution_count": null,
   "metadata": {},
   "outputs": [],
   "source": [
    "# Import the required libraries \n",
    "from scipy.sparse.linalg import ____\n",
    "import numpy as np\n",
    "\n",
    "# Decompose the matrix\n",
    "U, sigma, Vt = ____(user_ratings_centered)\n",
    "\n",
    "# Convert sigma into a diagonal matrix\n",
    "sigma = np.____(____)\n",
    "print(sigma)"
   ]
  },
  {
   "cell_type": "markdown",
   "metadata": {},
   "source": [
    "**Exercise:** Use numpy's dot product function to multiply `U` and `sigma` first, then the result by `Vt`. You will then be able add the average ratings for each row to find your final ratings."
   ]
  },
  {
   "cell_type": "code",
   "execution_count": null,
   "metadata": {},
   "outputs": [],
   "source": [
    "# Dot product of U and sigma\n",
    "U_sigma = np.____(____, ____)\n",
    "\n",
    "# Dot product of result and Vt\n",
    "U_sigma_Vt = np.dot(U_sigma, ____)\n",
    "\n",
    "# Add the row means back contained in avg_ratings\n",
    "uncentered_ratings = U_sigma_Vt + ____.____.reshape(-1, 1)\n",
    "\n",
    "# Create DataFrame of the results\n",
    "calc_pred_ratings_df = pd.____(uncentered_ratings, \n",
    "                                    index=user_ratings_df.____,\n",
    "                                    columns=user_ratings_df.____\n",
    "                                   )\n",
    "# Print both the recalculated matrix and the original \n",
    "print(calc_pred_ratings_df)\n",
    "print(original_df)"
   ]
  },
  {
   "cell_type": "markdown",
   "metadata": {},
   "source": [
    "**Exercise:** Using calc_pred_ratings_df that you generated in the last exercise, with all rows and columns filled, find the movies that User_5 is most likely to enjoy."
   ]
  },
  {
   "cell_type": "code",
   "execution_count": null,
   "metadata": {},
   "outputs": [],
   "source": [
    "# Sort the ratings of User 5 from high to low\n",
    "user_5_ratings = ____.____[____,:].____(____=____)\n",
    "\n",
    "print(user_5_ratings)"
   ]
  },
  {
   "cell_type": "markdown",
   "metadata": {},
   "source": [
    "## Validating your predictions\n",
    "### Hold-out sets\n",
    "What makes recommendation engines a little different when measuring predictions is that in more traditional machine learning models, you are trying to predict a single feature or column, but with recommendation engines, what you are trying to predict is far more inconsistent.\n",
    "Almost every user has reviewed different items, and each item has received reviews from different groups of users.\n",
    "For this reason, we cannot split our holdout set in the same way that we can for typical machine learning. In those cases, we would just split off a proportion of the row and use them to test our predictions as you see on the left.\n",
    "For recommendation engines, on the other hand, we need to remove a different chunk of the DataFrame, as seen on the right.\n",
    "\n",
    "<img src=\"image/79.PNG\" width=\"500\" height=\"100\">\n",
    "<img src=\"image/80.PNG\" width=\"500\" height=\"100\">\n",
    "\n",
    "### Separating the hold-out set\n",
    "```python\n",
    "actual_values = act_ratings_df.iloc[:20, :100].values\n",
    "act_ratings_df.iloc[:20, :100] = np.nan\n",
    "```\n",
    "Generate predictions as before.\n",
    "```python\n",
    "predicted_values = calc_pred_ratings_df.iloc[:20, :100].values\n",
    "```\n",
    "\n",
    "### Masking the hold-out set\n",
    "```python\n",
    "mask = ~np.isnan(actual_values)\n",
    "print(actual_values[mask])\n",
    "```\n",
    "answer:\n",
    "`[4. 4. 5. 3. 3. ...]`\n",
    "\n",
    "```python\n",
    "print(predicted_values[mask])```\n",
    "\n",
    "answer:\n",
    "`[3.76, 4.35, 4.95, 3.5869079 3.686337 ...]`\n",
    "\n",
    "### Introducing RMSE (root mean squared error)\n",
    "The metric most commonly used to measure how good a model is at predicting a recommendation is called root mean square error or RMSE for short.\n",
    "With RMSE, we first calculate how far from the ground truth each prediction was (this is the error part in RMSE).\n",
    "We then square this as we only care about how wrong it is, not in what direction.\n",
    "We then find the average square error.\n",
    "This gives us a good measure of how close a set of predictions are to the actual values, and is very useful to compare between models.\n",
    "<img src=\"image/81.PNG\" width=\"500\" height=\"100\">\n",
    "<img src=\"image/82.PNG\" width=\"500\" height=\"100\">\n",
    "\n",
    "\n",
    "### RMSE in Python\n",
    "\n",
    "```python\n",
    "from sklearn.metrics import mean_squared_error\n",
    "print(mean_squared_error(actual_values[mask],\n",
    "predicted_values[mask],\n",
    "squared=False))\n",
    "```\n",
    "answer:\n",
    "`3.6223997`\n",
    "\n",
    "<img src=\"https://i1.wp.com/www.firstplaceforhealth.com/wp-content/uploads/2020/03/train-your-brain-gif-2.gif?fit=1200%2C904&ssl=1\" width=\"300\" height=\"100\">\n",
    "\n",
    "**Exercise: Calculating RMSE**\n",
    "The following data has been loaded in the DataFrame predictions. Either manually, or using the Python console, calculate what the root mean square error (RMSE) of these predictions is.\n",
    "\n",
    "<img src=\"image/86.PNG\" width=\"500\" height=\"100\">\n"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": null,
   "metadata": {},
   "outputs": [],
   "source": []
  }
 ],
 "metadata": {
  "kernelspec": {
   "display_name": "Python 3",
   "language": "python",
   "name": "python3"
  },
  "language_info": {
   "codemirror_mode": {
    "name": "ipython",
    "version": 3
   },
   "file_extension": ".py",
   "mimetype": "text/x-python",
   "name": "python",
   "nbconvert_exporter": "python",
   "pygments_lexer": "ipython3",
   "version": "3.7.6"
  }
 },
 "nbformat": 4,
 "nbformat_minor": 4
}
